{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRuTTr4tv2gG"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0azlB97SKzds"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "api_token = {\"username\":\"swadeshkothari\",\"key\":\"99c6665f32c7cd47de90db4da39c0c3b\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaMJrH6tK2LK"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!kaggle competitions download -c 'airbnb-recruiting-new-user-bookings'\n",
    "\n",
    "!unzip 'train_users_2.csv'\n",
    "!unzip 'test_users.csv'\n",
    "!unzip 'sessions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmzB7kwDK86q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.cross_validation import StratifiedKFold \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,chi2\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from scipy.stats import skew,kurtosis\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "!pip install catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVV9i8cA5ozj"
   },
   "source": [
    "**Below Function will give the Predicted destination country for the given input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOJ6Tqr5jjZW"
   },
   "outputs": [],
   "source": [
    "#Function for prediction\n",
    "def final_fun_1(data_1_test,session_df):\n",
    "  data_1_test['date_account_created'] = pd.to_datetime(data_1_test['date_account_created'])\n",
    "  data_1_test['timestamp_first_active'] = pd.to_datetime((data_1_test.timestamp_first_active // 1000000), format='%Y%m%d')\n",
    "  data_1_test['weekday_account_created'] = data_1_test.date_account_created.dt.day_name()\n",
    "  data_1_test['day_account_created'] = data_1_test.date_account_created.dt.day\n",
    "  data_1_test['month_account_created'] = data_1_test.date_account_created.dt.month_name()\n",
    "  data_1_test['year_account_created'] = data_1_test.date_account_created.dt.year\n",
    "  data_1_test['weekday_first_active'] = data_1_test.timestamp_first_active.dt.day_name()\n",
    "  data_1_test['day_first_active'] = data_1_test.timestamp_first_active.dt.day\n",
    "  data_1_test['month_first_active'] = data_1_test.timestamp_first_active.dt.month_name()\n",
    "  data_1_test['year_first_active'] = data_1_test.timestamp_first_active.dt.year\n",
    "  data_1_test['time_gap']=(data_1_test['date_account_created'] - data_1_test['timestamp_first_active']).apply(lambda l: l.days)\n",
    "\n",
    "  data_1_test['date_first_booking'] = pd.to_datetime(data_1_test['date_first_booking'])\n",
    "  data_1_test['year_date_first_booking'] = data_1_test.date_first_booking.dt.year\n",
    "  #age field which contain values in 19's series and date_first_booking is not null then will take the difference of the age and date_first_booking year.\n",
    "  date_first_booking=data_1_test[(data_1_test.age>1000) & (data_1_test.age<2000) & ~data_1_test.date_first_booking.isnull() ][['year_date_first_booking']]\n",
    "  age=data_1_test[(data_1_test.age>1000) & (data_1_test.age<2000) & ~data_1_test.date_first_booking.isnull() ][['age']]\n",
    "  data_1_test.loc[((data_1_test.age>1000) & (data_1_test.age<2000) & (~data_1_test.date_first_booking.isnull())),'age' ]=date_first_booking['year_date_first_booking']-age['age']\n",
    "  #************************************************************************************************************************************************************\n",
    "  data_1_test['date_account_created'] = pd.to_datetime(data_1_test['date_account_created'])\n",
    "  data_1_test['year_date_account_created'] = data_1_test.date_account_created.dt.year\n",
    "  #age field which contain values in 19's series and date_account_created is not null then will take the difference of the age and date_account_created year.\n",
    "  year_date_account_created=data_1_test[(data_1_test.age>1000) & (data_1_test.age<2000) & ~data_1_test.date_account_created.isnull() ][['year_date_account_created']]\n",
    "  age=data_1_test[(data_1_test.age>1000) & (data_1_test.age<2000) & ~data_1_test.date_account_created.isnull() ][['age']]\n",
    "  data_1_test.loc[((data_1_test.age>1000) & (data_1_test.age<2000) & (~data_1_test.date_account_created.isnull())),'age' ]=year_date_account_created['year_date_account_created']-age['age']\n",
    "  data_1_test.loc[(data_1_test['age']<15) | (data_1_test['age']>95),'age']=np.nan\n",
    "  data_1_test=data_1_test.drop(['date_first_booking','date_account_created','timestamp_first_active','year_date_account_created','year_date_first_booking'],axis=1)\n",
    "  #Define age group\n",
    "  def set_age_group(x):\n",
    "      if x < 40:return 'Young'\n",
    "      elif x >=40 and x < 60:return 'Middle'\n",
    "      elif x >= 60 and x <= 125:return 'Old'\n",
    "      else:return 'Unknown_age'\n",
    "  data_1_test['age_group'] = data_1_test['age'].apply(set_age_group)\n",
    "  #Replace NAN to unknown.\n",
    "  data_1_test[['gender','signup_method','language','affiliate_channel','affiliate_provider','first_affiliate_tracked','signup_app','first_device_type','first_browser']]=data_1_test[['gender','signup_method' ,'language','affiliate_channel','affiliate_provider','first_affiliate_tracked','signup_app','first_device_type','first_browser']].fillna('-unknown-')\n",
    "  data_1_test[['signup_flow','age']]=data_1_test[['signup_flow','age']].fillna(0)\n",
    "  #weekday_account_created,day_account_created,month_account_created,weekday_first_active,day_first_active,month_first_active\n",
    "  user_data_test=data_1_test[['id','age','age_group','gender','signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked',\n",
    "         'signup_app', 'first_device_type', 'first_browser','weekday_account_created', 'day_account_created',\n",
    "         'month_account_created', 'year_account_created', 'weekday_first_active','day_first_active', 'month_first_active','year_first_active','time_gap']]\n",
    "  #Grouping multiple rows of dataframe with same user_id\n",
    "  session_df_group = session_df.groupby('user_id', as_index=False).agg(lambda x: x.tolist())\n",
    "  def conv_to_strings(items):\n",
    "      items = [ str(i) for i in items ]\n",
    "      items = [ re.sub('nan','',i) for i in items ] \n",
    "      items = ','.join(items)\n",
    "      return items\n",
    "  def conv_to_strings_unique(items):\n",
    "      items = [ str(i) for i in items ]\n",
    "      items = [ re.sub('nan','',i) for i in items ] \n",
    "      items = ','.join(set(items))\n",
    "      return items\n",
    "  def replace_nan_to_0(items):\n",
    "      items = [ 0 if math.isnan(i) else i for i in items ] \n",
    "      return items\n",
    "  session_df_group['action_unique_count'] = session_df_group['action'].apply(lambda i : len(np.unique(i)))\n",
    "  session_df_group['action_type_unique_count'] = session_df_group['action_type'].apply(lambda i : len(np.unique(i)))\n",
    "  session_df_group['action_detail_unique_count'] = session_df_group['action_detail'].apply(lambda i : len(np.unique(i)))\n",
    "  session_df_group['device_type_unique_count'] = session_df_group['device_type'].apply(lambda i : len(np.unique(i)))\n",
    "  session_df_group['action'] = session_df_group['action'].apply(conv_to_strings)\n",
    "  session_df_group['action_type'] = session_df_group['action_type'].apply(conv_to_strings)\n",
    "  session_df_group['action_detail'] = session_df_group['action_detail'].apply(conv_to_strings)\n",
    "  session_df_group['device_type'] = session_df_group['device_type'].apply(conv_to_strings_unique)\n",
    "  session_df_group['secs_elapsed'] = session_df_group['secs_elapsed'].apply(replace_nan_to_0)\n",
    "  session_df_group['secs_elapsed_min'] = session_df_group['secs_elapsed'].apply(lambda i : np.min(i))\n",
    "  session_df_group['secs_elapsed_max'] = session_df_group['secs_elapsed'].apply(lambda i : np.max(i))\n",
    "  session_df_group['secs_elapsed_mean'] = session_df_group['secs_elapsed'].apply(lambda i : np.mean(i))\n",
    "  session_df_group['secs_elapsed_median'] = session_df_group['secs_elapsed'].apply(lambda i : np.median(i))\n",
    "  session_df_group['secs_elapsed_std'] = session_df_group['secs_elapsed'].apply(lambda i : np.std(i))\n",
    "  session_df_group['secs_elapsed_var'] = session_df_group['secs_elapsed'].apply(lambda i : np.var(i))\n",
    "  session_df_group['secs_elapsed_skew'] = session_df_group['secs_elapsed'].apply(lambda i : skew(i))\n",
    "  session_df_group['secs_elapsed_kurtosis'] = session_df_group['secs_elapsed'].apply(lambda i : kurtosis(i))\n",
    "  session_df_group['secs_elapsed'] = session_df_group['secs_elapsed'].apply(lambda i : np.sum(i))\n",
    "  final_df_test = data_1_test.merge(session_df_group, left_on='id', right_on='user_id', how='left')\n",
    "  #Applying Count Vectorizer (BOW and TFIDF)\n",
    "  #def tokens(x):return x.split(',')\n",
    "  categorical_columns=['action','action_type','action_detail','device_type','age_group','gender','signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked','signup_app', 'first_device_type', 'first_browser','weekday_account_created', 'day_account_created','month_account_created', 'weekday_first_active','day_first_active', 'month_first_active']\n",
    "  numerical_column=['action_unique_count','action_type_unique_count','action_detail_unique_count','device_type_unique_count','age','year_account_created','year_first_active','time_gap','secs_elapsed','secs_elapsed_min','secs_elapsed_max','secs_elapsed_mean','secs_elapsed_median','secs_elapsed_std','secs_elapsed_var','secs_elapsed_skew','secs_elapsed_kurtosis'] \n",
    "  test=sp.coo_matrix((0,0))\n",
    "  for i in categorical_columns:\n",
    "    with open('drive/My Drive/vectorizer/cnt_vct_'+i+'.pkl', \"rb\" ) as f:\n",
    "      cnt_vct=pkl.load(f)\n",
    "    f.close()\n",
    "    categorical_columns_cnt_vct_test=cnt_vct.transform(final_df_test[i].apply(lambda j :str(j)))\n",
    "    test=sp.hstack((test, categorical_columns_cnt_vct_test))\n",
    "  final_df_test=sp.hstack((test,sp.csr_matrix(final_df_test[numerical_column])))\n",
    "  with open('drive/My Drive/label_encoder/label_encoder.pkl', \"rb\" ) as f:\n",
    "    le=pkl.load(f)\n",
    "  f.close()\n",
    "  with open('drive/My Drive/final_model/final_model.pkl', \"rb\" ) as f:\n",
    "    cat=pkl.load(f)\n",
    "  f.close()\n",
    "\n",
    "  #test_df = pd.read_csv('test_users.csv')\n",
    "  test_id = data_1_test['id'].values\n",
    "  pred = cat.predict_proba(final_df_test)\n",
    "\n",
    "  ids = []\n",
    "  countries = []\n",
    "  # Taking the 5 classes with highest probabilities\n",
    "  for i in range(len(test_id)):\n",
    "      idx = test_id[i]\n",
    "      ids += [idx] * 5\n",
    "      countries += le.inverse_transform(np.argsort(pred[i])[::-1][:5]).tolist()\n",
    "  # Create submission\n",
    "  submission = pd.DataFrame({\"id\" : ids,\"country\" : countries})\n",
    "  return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK1V9LbXjo5V"
   },
   "outputs": [],
   "source": [
    "data_1_test=pd.read_csv('test_users.csv')\n",
    "session_df = pd.read_csv(\"sessions.csv\")\n",
    "submission=final_fun_1(data_1_test,session_df)\n",
    "submission.to_csv('final_submission1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Uh-2Atk5fXf"
   },
   "source": [
    "**Below Function will give the NDGC Score for the given input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HQCF-XpU3afL"
   },
   "outputs": [],
   "source": [
    "#Function for NDGC Score\n",
    "def final_fun_2(data_1_test,session_df):\n",
    "  data_1_test['date_account_created'] = pd.to_datetime(data_1_test['date_account_created'])\n",
    "  data_1_test['timestamp_first_active'] = pd.to_datetime((data_1_test.timestamp_first_active // 1000000), format='%Y%m%d')\n",
    "  data_1_test['weekday_account_created'] = data_1_test.date_account_created.dt.day_name()\n",
    "  data_1_test['day_account_created'] = data_1_test.date_account_created.dt.day\n",
    "  data_1_test['month_account_created'] = data_1_test.date_account_created.dt.month_name()\n",
    "  data_1_test['year_account_created'] = data_1_test.date_account_created.dt.year\n",
    "  data_1_test['weekday_first_active'] = data_1_test.timestamp_first_active.dt.day_name()\n",
    "  data_1_test['day_first_active'] = data_1_test.timestamp_first_active.dt.day\n",
    "  data_1_test['month_first_active'] = data_1_test.timestamp_first_active.dt.month_name()\n",
    "  data_1_test['year_first_active'] = data_1_test.timestamp_first_active.dt.year\n",
    "  data_1_test['time_gap']=(data_1_test['date_account_created'] - data_1_test['timestamp_first_active']).apply(lambda l: l.days)\n",
    "\n",
    "  data_1_test['date_first_booking'] = pd.to_datetime(data_1_test['date_first_booking'])\n",
    "  data_1_test['year_date_first_booking'] = data_1_test.date_first_booking.dt.year\n",
    "  #age field which contain values in 19's series and date_first_booking is not null then will take the difference of the age and date_first_booking year.\n",
    "  date_first_booking=data_1_test[(data_1_test.age>1000) & (data_1_test.age<2000) & ~data_1_test.date_first_booking.isnull() ][['year_date_first_booking']]\n",
    "  age=data_1_test[(data_1_test.age>1000) & (data_1_test.age<2000) & ~data_1_test.date_first_booking.isnull() ][['age']]\n",
    "  data_1_test.loc[((data_1_test.age>1000) & (data_1_test.age<2000) & (~data_1_test.date_first_booking.isnull())),'age' ]=date_first_booking['year_date_first_booking']-age['age']\n",
    "  #************************************************************************************************************************************************************\n",
    "  data_1_test['date_account_created'] = pd.to_datetime(data_1_test['date_account_created'])\n",
    "  data_1_test['year_date_account_created'] = data_1_test.date_account_created.dt.year\n",
    "  #age field which contain values in 19's series and date_account_created is not null then will take the difference of the age and date_account_created year.\n",
    "  year_date_account_created=data_1_test[(data_1_test.age>1000) & (data_1_test.age<2000) & ~data_1_test.date_account_created.isnull() ][['year_date_account_created']]\n",
    "  age=data_1_test[(data_1_test.age>1000) & (data_1_test.age<2000) & ~data_1_test.date_account_created.isnull() ][['age']]\n",
    "  data_1_test.loc[((data_1_test.age>1000) & (data_1_test.age<2000) & (~data_1_test.date_account_created.isnull())),'age' ]=year_date_account_created['year_date_account_created']-age['age']\n",
    "  data_1_test.loc[(data_1_test['age']<15) | (data_1_test['age']>95),'age']=np.nan\n",
    "  data_1_test=data_1_test.drop(['date_first_booking','date_account_created','timestamp_first_active','year_date_account_created','year_date_first_booking'],axis=1)\n",
    "  #Define age group\n",
    "  def set_age_group(x):\n",
    "      if x < 40:\n",
    "          return 'Young'\n",
    "      elif x >=40 and x < 60:\n",
    "          return 'Middle'\n",
    "      elif x >= 60 and x <= 125:\n",
    "          return 'Old'\n",
    "      else:\n",
    "          return 'Unknown_age'\n",
    "  data_1_test['age_group'] = data_1_test['age'].apply(set_age_group)\n",
    "  #Replace NAN to unknown.\n",
    "  data_1_test[['gender','signup_method','language','affiliate_channel','affiliate_provider','first_affiliate_tracked','signup_app','first_device_type','first_browser']]=data_1_test[['gender','signup_method' ,'language','affiliate_channel','affiliate_provider','first_affiliate_tracked','signup_app','first_device_type','first_browser']].fillna('-unknown-')\n",
    "  data_1_test[['signup_flow','age']]=data_1_test[['signup_flow','age']].fillna(0)\n",
    "\n",
    "  #weekday_account_created,day_account_created,month_account_created,weekday_first_active,day_first_active,month_first_active\n",
    "  user_data_test=data_1_test[['id','age','age_group','gender','signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked',\n",
    "         'signup_app', 'first_device_type', 'first_browser','weekday_account_created', 'day_account_created',\n",
    "         'month_account_created', 'year_account_created', 'weekday_first_active','day_first_active', 'month_first_active', \n",
    "          'year_first_active','time_gap']]\n",
    "\n",
    "\n",
    "  #Grouping multiple rows of dataframe with same user_id\n",
    "  session_df_group = session_df.groupby('user_id', as_index=False).agg(lambda x: x.tolist())\n",
    "\n",
    "  def conv_to_strings(items):\n",
    "      items = [ str(i) for i in items ]\n",
    "      items = [ re.sub('nan','',i) for i in items ] \n",
    "      items = ','.join(items)\n",
    "      return items\n",
    "  def conv_to_strings_unique(items):\n",
    "      items = [ str(i) for i in items ]\n",
    "      items = [ re.sub('nan','',i) for i in items ] \n",
    "      items = ','.join(set(items))\n",
    "      return items\n",
    "  def replace_nan_to_0(items):\n",
    "      items = [ 0 if math.isnan(i) else i for i in items ] \n",
    "      return items\n",
    "\n",
    "\n",
    "  session_df_group['action_unique_count'] = session_df_group['action'].apply(lambda i : len(np.unique(i)))\n",
    "  session_df_group['action_type_unique_count'] = session_df_group['action_type'].apply(lambda i : len(np.unique(i)))\n",
    "  session_df_group['action_detail_unique_count'] = session_df_group['action_detail'].apply(lambda i : len(np.unique(i)))\n",
    "  session_df_group['device_type_unique_count'] = session_df_group['device_type'].apply(lambda i : len(np.unique(i)))\n",
    "\n",
    "  session_df_group['action'] = session_df_group['action'].apply(conv_to_strings)\n",
    "  session_df_group['action_type'] = session_df_group['action_type'].apply(conv_to_strings)\n",
    "  session_df_group['action_detail'] = session_df_group['action_detail'].apply(conv_to_strings)\n",
    "  session_df_group['device_type'] = session_df_group['device_type'].apply(conv_to_strings_unique)\n",
    "\n",
    "  session_df_group['secs_elapsed'] = session_df_group['secs_elapsed'].apply(replace_nan_to_0)\n",
    "  session_df_group['secs_elapsed_min'] = session_df_group['secs_elapsed'].apply(lambda i : np.min(i))\n",
    "  session_df_group['secs_elapsed_max'] = session_df_group['secs_elapsed'].apply(lambda i : np.max(i))\n",
    "  session_df_group['secs_elapsed_mean'] = session_df_group['secs_elapsed'].apply(lambda i : np.mean(i))\n",
    "  session_df_group['secs_elapsed_median'] = session_df_group['secs_elapsed'].apply(lambda i : np.median(i))\n",
    "  session_df_group['secs_elapsed_std'] = session_df_group['secs_elapsed'].apply(lambda i : np.std(i))\n",
    "  session_df_group['secs_elapsed_var'] = session_df_group['secs_elapsed'].apply(lambda i : np.var(i))\n",
    "  session_df_group['secs_elapsed_skew'] = session_df_group['secs_elapsed'].apply(lambda i : skew(i))\n",
    "  session_df_group['secs_elapsed_kurtosis'] = session_df_group['secs_elapsed'].apply(lambda i : kurtosis(i))\n",
    "  session_df_group['secs_elapsed'] = session_df_group['secs_elapsed'].apply(lambda i : np.sum(i))\n",
    "  final_df_test = data_1_test.merge(session_df_group, left_on='id', right_on='user_id', how='inner')\n",
    "\n",
    "  #Applying Count Vectorizer (BOW and TFIDF)\n",
    "  #def tokens(x):return x.split(',')\n",
    "  label=final_df_test['country_destination']\n",
    "  categorical_columns=['action','action_type','action_detail','device_type','age_group','gender','signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked','signup_app', 'first_device_type', 'first_browser','weekday_account_created', 'day_account_created','month_account_created', 'weekday_first_active','day_first_active', 'month_first_active']\n",
    "  numerical_column=['action_unique_count','action_type_unique_count','action_detail_unique_count','device_type_unique_count','age','year_account_created','year_first_active','time_gap','secs_elapsed','secs_elapsed_min','secs_elapsed_max','secs_elapsed_mean','secs_elapsed_median','secs_elapsed_std','secs_elapsed_var','secs_elapsed_skew','secs_elapsed_kurtosis'] \n",
    "  test=sp.coo_matrix((0,0))\n",
    "  for i in categorical_columns:\n",
    "    with open('drive/My Drive/vectorizer/cnt_vct_'+i+'.pkl', \"rb\" ) as f:\n",
    "      cnt_vct=pkl.load(f)\n",
    "    f.close()\n",
    "    categorical_columns_cnt_vct_test=cnt_vct.transform(final_df_test[i].apply(lambda j :str(j)))\n",
    "    test=sp.hstack((test, categorical_columns_cnt_vct_test))\n",
    "  final_df_test=sp.hstack((test,sp.csr_matrix(final_df_test[numerical_column])))\n",
    "  with open('drive/My Drive/label_encoder/label_encoder.pkl', \"rb\" ) as f:\n",
    "    le=pkl.load(f)\n",
    "  f.close()\n",
    "  with open('drive/My Drive/final_model/final_model.pkl', \"rb\" ) as f:\n",
    "    cat=pkl.load(f)\n",
    "  f.close()\n",
    "  \n",
    "  test_label=le.transform(label)\n",
    "  test_predition = cat.predict_proba(final_df_test)\n",
    "  \n",
    "  def dcg_score(y_true, y_score, k=5):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gain = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "  def ndcg_score(ground_truth, predictions, k=5):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(predictions.shape[1]))\n",
    "    T = lb.transform(ground_truth)\n",
    "    scores = []\n",
    "    # Iterate over each y_true and compute the DCG score\n",
    "    for y_true, y_score in zip(T, predictions):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        score = float(actual) / float(best)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "  score=ndcg_score(test_label, test_predition, k=5)\n",
    "  \n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IN9t9Ke78Iq",
    "outputId": "8f76ed6f-6f57-495d-d240-383fd30127bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDGC :  0.8781705149977449\n"
     ]
    }
   ],
   "source": [
    "data_1_test=pd.read_csv('test_users.csv')\n",
    "data_1_train=pd.read_csv('train_users_2.csv')\n",
    "session_df = pd.read_csv(\"sessions.csv\")\n",
    "label=data_1_train['country_destination']\n",
    "score=final_fun_2(data_1_train,session_df)\n",
    "print('NDGC : ',score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
